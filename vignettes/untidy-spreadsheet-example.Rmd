---
title: "Untidy Spreadsheet Data"
author: "David W. Body"
date: "4/16/2019"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Untidy spreadsheet}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Working with untidy spreadsheet data

I created an untidy spreadsheet of simulated sales data for a company with multiple locations in Indiana. The document presents several challenges we often encounter with real spreadsheets, including:

* Separate sheets (tabs) for groups in the data (in this case year)
* Multiple header rows, including one with "merged cells"
* Repeated headers within the same sheet
* Number of columns varies by sheet

The document contains 10 sheets (tabs), so coding them one-by-one isn't very appealing. We'll want to automate processing the sheets.

### Manually process a representative sheet

Before we create an automated solution, let's start by manually processing a representative sheet so we can figure out what we need to automate.

Open the document in your favorite spreadsheet software and you'll see that the first few sheets contain data for just one location, Noblesville. Later sheets contain data for more than one location. Sheet 7 (for 2016) looks like a good one to work with because it has data for multiple locations.

Notice that the location names are in cells that have been merged and centered, and there are price and quantity columns for each of the locations. Also notice that data for the last location, South Bend, is missing for the first several rows.

```{r echo=FALSE}
knitr::include_graphics("untidy-spreadsheet.png")
```

Let's start by loading the `tidyverse` and `readxl` packages and setting `file_name` to the path to the spreadsheet. Since the spreadsheet is part of the `untidydata2` package, we use `system.file` to return a path to the spreadsheet.

```{r}
library(tidyverse)
library(readxl)

file_name <- system.file("messydata", "combined_sales.xlsx", package="untidydata2")

```


First, let's get a list of locations contained in sheet for 2016 using `readxl::read_excel`. We can read just the first row by setting `n_max = 0` and then passing the result to `names`.

```{r message=FALSE}
read_excel(file_name, sheet = "2016", n_max = 0) %>%
  names()
```

The merged cells are coming back with names of "...2", "...4", etc. We can remove them from the list with `stringr::str_subset` and a regular expressing that matches "..." at the beginning of a string.

```{r message=FALSE}
locations <- read_excel(file_name, sheet = "2016", n_max = 0) %>%
  names() %>%
  str_subset("^\\.{3}", negate = TRUE)

locations
```

Now we have a character vector of locations. Let's read the rest of the data from the 2016 sheet, skipping the first row this time.

```{r message=FALSE}
read_excel(file_name, sheet = "2016", skip = 1)
```
We're making progress but we need to do something to associate the `price...` and `quantity...` variables with their correct locations. Later we will want to have a separate variable for location, but for now let's rename our `price` and `quantity` variables to `price_Noblesville`, `quantity_Noblesville`, `price_Bloomington`, `quantity_Bloomington`, etc.
 
Notice that `price...3` and `quantity...4` correspond to the first location, `price...5` and `quantity...6` correspond to the second location, etc. Let's write a function to give us the corresponding location index for each value 3, 4, 5, 6, etc.

```{r message=FALSE}
loc_index <- function(n) {
  floor((n-1)/2)
}
```

Now let's verify that this works.

```{r}
loc_index(3:10)
```

Yes it does.

Now we can write a function to calculate a new name for each column we want to rename.

```{r}
new_name <- function(columns) {
  num <- as.integer(str_extract(columns, "\\d+"))
  location <- locations[loc_index(num)]
  str_replace(columns, paste0("...", num), paste0("_", location))
}
```

And let's verify that this works.

```{r}
new_name(c("price...3", "quantity...4", "price...5", "quantity...6"))
```

Okay. Now we're ready to read the spreadsheet and rename the columns.

```{r message=FALSE}
data_2016 <- read_excel(file_name, sheet = "2016", skip = 1) %>%
  rename_at(vars(contains("...")), new_name)

data_2016
```

### The trickiest part of tidying this data

Finally, let's extract location, price, and quanity variables. This is the trickiest part of tidying this data because it involves gathering multiple variables and then spreading them again. We also add the year and order the variables.

```{r}
data_2016 %>%
  gather(-month, -day, key = "label", value = "value") %>%
  separate(label, into = c("type", "location"), sep = "_") %>%
  spread(key = "type", value = "value") %>%
  filter(!is.na(price) | !is.na(quantity)) %>%
  mutate(year = 2016) %>%
  select(year, month, day, location, price, quantity)
```

Whoa, there is a lot going on there! Let's do that again and look at the intermediate results.

### One more time with intermediate results

First we gather the location/price/quantity columns into `label` and `value` variables.

```{r}
step1 <- data_2016 %>%
  gather(-month, -day, key = "label", value = "value")
head(step1)
tail(step1)
```

Then we can separate `location` into its own variable, while maintaining a `type` variable to distinguish `price` and `quantity`.

```{r}
step2 <- step1 %>%
  separate(label, into = c("type", "location"), sep = "_")
head(step2)
tail(step2)
```

Now we can move `price` and `quantity` into their own variables using `tidyr::spread`.

```{r}
step3 <- step2 %>%
  spread(key = "type", value = "value")
step3
```

Finally, we filter out NA values, add the year, and reorder the variables.

```{r}
step3 %>%
  filter(!is.na(price) | !is.na(quantity)) %>%
  mutate(year = 2016) %>%
  select(year, month, day, location, price, quantity)
```

This looks good, but it's just one sheet. Now let's write some code to process the whole spreadsheet document.

### A function to read any specified sheet from the document

Here is a function that combines everything we've done above to read any specified sheet from the document. It also handles the simpler special case when a sheet contains only a single location.

```{r message=FALSE}
read_sheet <- function(file_name, sheet) {
  locations <- read_excel(file_name, sheet = sheet, n_max = 0) %>%
    names() %>%
    str_subset("^\\.{3}", negate = TRUE)

  df <- read_excel(file_name, sheet = sheet, skip = 1)
  
  if (length(locations) == 1) {
    df <- df %>%
      mutate(location = locations[1])
  } else {
    loc_index <- function(n) {
      floor((n-1)/2)
    }
    
    new_name <- function(columns) {
      num <- as.integer(str_extract(columns, "\\d+"))
      location <- locations[loc_index(num)]
      str_replace(columns, paste0("...", num), paste0("_", location))
    }
    
    df <- df %>%
      rename_at(vars(contains("...")), new_name) %>%
      gather(-month, -day, key = "label", value = "value") %>%
      separate(label, into = c("type", "location"), sep = "_") %>%
      spread(key = "type", value = "value")
  }
  
  df <- df %>%
    filter(!is.na(price) | !is.na(quantity)) %>%
    mutate(year = as.integer(sheet)) %>%
    select(year, month, day, location, price, quantity)

  df
}

read_sheet(file_name, "2016")
```

The hard part is done. Now let's read and combine all the sheets.

### Reading all the sheets into one tidy data frame

Start with a vector of sheet names for the spreadsheet document.

```{r}
sheets <- excel_sheets(file_name)
sheets
```

Now we use `purrr::map_dfr` to read a data frame for each sheet and combine them into a single data frame.

```{r, message=FALSE}
combined_df <- map_dfr(sheets, ~ read_sheet(file_name, .x))
head(combined_df)
tail(combined_df)
```

At least the last step was easy!

We now have a tidy data frame containing all of the data from the spreadsheet.

## Bonus material: Simpson's Paradox

The main point of this vignette is to show how to tidy data an untidy spreadsheet. But let's take just a few more minutes to look at the data.

First, let's plot `price` and `quantity`.

```{r}
combined_df %>%
  ggplot(aes(price, quantity)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Based on this plot, it appears higher prices are associated with greater quantities sold. Should management consider raising prices? Based on this plot, it doesn't look like higher prices would cause reduced sales.

If we look at individual locations, we see something quite different.

```{r}
combined_df %>%
  ggplot(aes(price, quantity)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~ location)
```

When locations are considered separately, higher prices are associated with lower quantities sold, as we would normally expect. The only exception appears to be Noblesville. Noblesville is this fictitious company's oldest location. We have data for Noblesville from 2010 to the present.

Let's look at Noblesville by year.

```{r}
combined_df %>%
  filter(location == "Noblesville") %>%
  ggplot(aes(price, quantity)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~ year)
```

So higher prices are associated with lower quantities even for Noblesville if we look at shorter time periods like a year.

This phenomen where an association appears to exist in data, but disappears or even reverses when groups are analyzed separately, is known as [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson's_paradox). You can read more about at the link.
